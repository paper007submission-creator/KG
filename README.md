# Graph-Based Query Performance Prediction (QPP)

This repository provides a PyTorch Geometric (PyG)-based implementation of a **Graph Neural Network (GNN)** framework for **Query Performance Prediction (QPP)**.  
using similarity graphs constructed from dense embeddings of queries.  It predicts query difficulty or effectiveness metrics such as **MAP**.

---

The code uses:
- **PyTorch Geometric** for message passing
- **Sentence-Transformers** for dense query embeddings
- **FAISS** for nearest neighbor search between queries
- **Huber loss** for robust regression training

---

## ğŸ§© Graph Structure

Each node represents a **query**, and edges encode **semantic similarity** between queries based on their dense embeddings.

- **Nodes:** Queries  
- **Edges:** Queryâ€“Query similarities (`('query', 'similar', 'query')`)  
- **Edge weights:** Cosine similarity scores normalized by row-wise softmax

The model passes messages between similar queries to predict per-query performance.

---

## âš™ï¸ Model Components

### 1. **BaseGraphConv**
A simple 2-layer GCN (GraphConv) architecture operating only on Qâ€“Q edges:

```python
class BaseGraphConv(nn.Module):
    def __init__(self, hid: int):
        super().__init__()
        self.conv1 = GraphConv(-1, hid, aggr='add')
        self.conv2 = GraphConv(hid, hid, aggr='add')
        self.act = nn.ReLU()
        self.drop = nn.Dropout(0.2)
```


# ğŸ§  Graph-Based Query Performance Prediction (QPP) â€” Query-Only Pipeline

This repository provides the full experimental pipeline for **graph-based Query Performance Prediction (QPP)** using **queryâ€“query (Qâ€“Q)** relations under the **BM25 baseline**.  
The pipeline is designed to run efficiently on an **HPC cluster** via the provided **SLURM batch script** (`KGQPP_qq.sh`).

---

## ğŸš€ Overview

This system builds and trains a **Graph Neural Network (GNN)** to estimate query effectiveness (e.g., MAP or NDCG) using **semantic relationships between queries**.  
Each query node connects to its top-K similar neighbors based on dense embeddings.

### Pipeline Steps
1. Encode queries using Sentence-Transformers  
2. Build Qâ€“Q nearest neighbor graphs using FAISS  
3. Generate JSON datasets for training and evaluation  
4. Convert them into PyTorch Geometric `.pt` graphs  
5. Train a GCN (or GAT) for performance prediction  
6. Evaluate using Pearson, Spearman, and Kendall correlations  

---

## ğŸ“ Project Structure

GNN-QPP/
â”‚
â”œâ”€â”€ KGQPP_qq.sh                          # SLURM bash script (main pipeline)
â”œâ”€â”€ sim_search.py                        # Compute Qâ€“Q similarity with FAISS
â”œâ”€â”€ make_dataset_qq.py                   # Build MotherDataset JSONs
â”œâ”€â”€ dataset_builder_qq.py                # Convert JSONs to PyG .pt graphs
â”œâ”€â”€ train_model_GCN_qq.py                # Train model (GCN backbone)
â”œâ”€â”€ train_model_GCN_qq_improved.py       # Alternative version with enhancements
â”œâ”€â”€ correlation.py                       # Compute correlation metrics
â”‚
â””â”€â”€ dataset/
â”œâ”€â”€ v1/                              # Query TSVs and supporting files
â”œâ”€â”€ NNQ/                             # Nearest neighbor JSONs
â”œâ”€â”€ Bm25/eval/                       # Evaluation metrics (MAP, NDCG)
â””â”€â”€ KGQPP/V1_BM25_OnlyQ/             # Graphs, checkpoints, results
